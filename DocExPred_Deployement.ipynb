{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DocExPred-Deployement.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEpE2cn0+qkwUWMfw9NpdY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndHVW4Fzg_3-"
      },
      "source": [
        "# Deployement of Document Extraction and Classification via importing the previously trained and exported models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPwTN6ULt1Lb"
      },
      "source": [
        "#pred_sentences=[\"\",\"XE Money Transfer is a Good money transfer service (7.5/10) recommended by Monito. It's a fully licensed and authorized money transfer provider, and has a very high number and percentage of positive customer reviews on TrustPilot (10/10). Setting up your transfer with XE Money Transfer is quick and easy (8.3/10). However, the exchange rate margin makes up most of the total cost of your transfer with XE Money Transfer service (3.8/10).\"]\n",
        "#pred_sentences=[\"\",\"Fraud is new to the Top Ten list, with many consumers complaining about a range of scams including work-at-home schemes and fraudulent sweepstakes and lotteries. One man from Ohio reported being thrilled when someone called to tell him he'd won $850,000 in a sweepstakes. All he had to do to claim the prize was transfer $220 to someone in Jamaica to pay the fees on his winnings. After he transferred that money, they asked him for another $650. It was after that the Ohio man discovered that the person was not really from a sweepstakes company and he was not really a winner. \\\"Fraud is an especially challenging problem because scammers often target U.S. consumers from foreign countries, making law enforcement difficult,\\\" said Anna Huddleston-Aycock, President of North American Consumer Protection Investigators.\"]\n",
        "#pred_sentences=[\"\",\"My password is uhiudbdk and also pls transfer 200 to it. You can take my email id: XXXX. Details of my credit card:XXXX. Very important document. COnfidential. Take note.\"]\n",
        "#pred_sentences=[\"\",\"For the sample from materials sciences, directed at an internal fellowship, the one-page essay has an especially difficult task: The writer must persuade those who already know him (and thus know both his strengths and limitations) that he is worthy of internal funds to help him continue his graduate education. He attempts this by first citing the specific goal of his research group, followed by a brief summary of the literature related to this topic, then ending with a summary of his own research and lab experience.\"]\n",
        "imp=[\"confidential\",\"internal use only\", \"not for distribution\", \"not for public distribution\", \"classified \", \"password\",\"passwords\",\"email\",\"important\",\"very important\",\"database\",\"atm pin\",\"cvv\",\"mpin\",\"dataset\"]\n",
        "#len(pred_sentences[1])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4vIjW8Kb2l2"
      },
      "source": [
        "### Downloading the trained bert estimator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQUfGtPuQGX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed21ce4-2c55-4ed1-c729-d0d8d5ed6d54"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "\n",
        "!gdown --id 1yLJnjYNjranb5zAXXr_q883trF1aAxZx"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yLJnjYNjranb5zAXXr_q883trF1aAxZx\n",
            "To: /content/bert-model2.zip\n",
            "408MB [00:03, 122MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAya2uuHb_9k"
      },
      "source": [
        "### Downloading the saved LSTM keras model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEmFP3ZyBScl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2177e1c2-8d08-4a6f-ddc7-9e264e79e2dc"
      },
      "source": [
        "#!cp \"/content/gdrive/My Drive/model2.h5\" \"/content\"\n",
        "\n",
        "!gdown --id 1--kvG6PlwLSmD834J0Bw9Fsa7efK8FJk"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--kvG6PlwLSmD834J0Bw9Fsa7efK8FJk\n",
            "To: /content/model2.h5\n",
            "\r0.00B [00:00, ?B/s]\r4.24MB [00:00, 132MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeOLcQF0Q3JY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1815a28-0461-4ec9-b49b-3236d1c487ba"
      },
      "source": [
        "!unzip /content/bert-model2.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/bert-model2.zip\n",
            "   creating: bert_news_category/1597156650/\n",
            "  inflating: bert_news_category/1597156650/saved_model.pb  \n",
            "   creating: bert_news_category/1597156650/variables/\n",
            "  inflating: bert_news_category/1597156650/variables/variables.index  \n",
            "  inflating: bert_news_category/1597156650/variables/variables.data-00000-of-00001  \n",
            "   creating: bert_news_category/1597156650/assets/\n",
            "  inflating: bert_news_category/1597156650/assets/vocab.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DplwB89unOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37ee466-bcc8-4209-f96c-5aac4b43d883"
      },
      "source": [
        "%tensorflow_version 1.x magic"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x magic`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTnrSltdeYW6"
      },
      "source": [
        "Loading BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU-QE8BJRXQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012c225c-6376-4832-cafa-447dbbd7bb67"
      },
      "source": [
        "from tensorflow.contrib import predictor\n",
        "predict_fn = predictor.from_saved_model('/content/bert_news_category/1597156650')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "INFO:tensorflow:Restoring parameters from /content/bert_news_category/1597156650/variables/variables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1VxH2iFuqhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff35d7b3-081e-4acc-9aa9-ddef2924fd5d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(1337)\n",
        "from keras import Sequential\n",
        "from keras.utils import Sequence\n",
        "from keras.layers import LSTM, Dense, Masking\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Dense, Input, concatenate, Layer, Lambda, Dropout, Activation\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, TensorBoard\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw372tuSvTDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45056faf-2720-406e-8d63-2bf53237993c"
      },
      "source": [
        "from keras.models import load_model\n",
        "!pip install h5py==2.10.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDloEIEieegw"
      },
      "source": [
        "Loading LSTM keras model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG-xJ9yWuwxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3595d005-84d8-48a2-b424-6b38ed4fe05b"
      },
      "source": [
        "model = load_model('/content/model2.h5')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thorVPTwiHbr"
      },
      "source": [
        "### Function for splitting the document data into smaller chunks, as the BERT transformer model is limited to 512 tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwRsDK8fvsHa"
      },
      "source": [
        "def get_split(text1):\n",
        "  l_total = []\n",
        "  l_parcial = []\n",
        "  if len(text1.split())//150 >0:\n",
        "    n = len(text1.split())//150\n",
        "  else: \n",
        "    n = 1\n",
        "  for w in range(n):\n",
        "    if w == 0:\n",
        "      l_parcial = text1.split()[:200]\n",
        "      l_total.append(\" \".join(l_parcial))\n",
        "    else:\n",
        "      l_parcial = text1.split()[w*150:w*150 + 200]\n",
        "      l_total.append(\" \".join(l_parcial))\n",
        "  return l_total"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4NdMbwXuFOO"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0wXEFg7vxay"
      },
      "source": [
        "DATA_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgYj6x_9uISF"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdu3PIC1ws0W"
      },
      "source": [
        "import re\n",
        "def clean_txt(text):\n",
        "  text = re.sub(\"'\", \"\",text)\n",
        "  text=re.sub(\"(\\\\W)+\",\" \",text)    \n",
        "  return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBOdJygKx1ph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0d070f-3f74-42c1-b44b-a3202514de36"
      },
      "source": [
        "lab = np.array([0,1, 2, 3, 4, 5,6,7,8,9])\n",
        "DATA_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'\n",
        "# The list containing all the classes (train['SECTION'].unique())\n",
        "label_list = [x for x in np.unique(lab)]\n",
        "label_list"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf7de3__zCdz"
      },
      "source": [
        "MAX_SEQ_LENGTH=200"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzZIuKkdyIi5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "185620c7-800e-4c8a-fde2-cafe3ace4841"
      },
      "source": [
        "!pip install bert-tensorflow==1.0.1\n",
        "!pip install textract\n",
        "!pip install PyPDF2"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow==1.0.1) (1.15.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n",
            "Collecting textract\n",
            "  Downloading https://files.pythonhosted.org/packages/32/31/ef9451e6e48a1a57e337c5f20d4ef58c1a13d91560d2574c738b1320bb8d/textract-1.6.3-py3-none-any.whl\n",
            "Collecting argcomplete==1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/82/f44c9661e479207348a979b1f6f063625d11dc4ca6256af053719bbb0124/argcomplete-1.10.0-py2.py3-none-any.whl\n",
            "Collecting SpeechRecognition==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 131kB/s \n",
            "\u001b[?25hCollecting EbookLib==0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/38/7d6ab2e569a9165249619d73b7bc6be0e713a899a3bc2513814b6598a84c/EbookLib-0.17.1.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 27.7MB/s \n",
            "\u001b[?25hCollecting python-pptx==0.6.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/86/eb979f7b0333ec769041aae36df8b9f1bd8bea5bbad44620663890dce561/python-pptx-0.6.18.tar.gz (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 16.7MB/s \n",
            "\u001b[?25hCollecting extract-msg==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/90/84485a914ed90adb5e87df17e626be04162fbba146dfecf34643659a4633/extract_msg-0.23.1-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20181108\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/6e8746e6965d1a7ea8e97253e3d79e625da5547e8f376f88de5d024bacb9/pdfminer.six-20181108-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 18.4MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.6MB/s \n",
            "\u001b[?25hCollecting six==1.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting docx2txt==0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from textract) (3.0.4)\n",
            "Collecting xlrd==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/63576a1a001752e34bf8ea62e367997530dc553b689356b9879339cf45a4/xlrd-1.2.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from EbookLib==0.17.1->textract) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx==0.6.18->textract) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/ce/74fd8d638a5b82ea0c6f08a5978f741c2655a38c3d6e82f73a0f084377e6/XlsxWriter-1.4.3-py2.py3-none-any.whl (149kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 50.9MB/s \n",
            "\u001b[?25hCollecting imapclient==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/39/e1c2c2c6e2356ab6ea81fcfc0a74b044b311d6a91a45300811d9a6077ef7/IMAPClient-2.1.0-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.7/dist-packages (from extract-msg==0.23.1->textract) (1.5.1)\n",
            "Collecting olefile==0.46\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 52.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20181108->textract) (2.4.0)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/16/9627ab0493894a11c68e46000dbcc82f578c8ff06bc2980dcd016aea9bd3/pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 12.7MB/s \n",
            "\u001b[?25hCollecting soupsieve>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2018.9)\n",
            "Building wheels for collected packages: EbookLib, python-pptx, docx2txt, olefile\n",
            "  Building wheel for EbookLib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EbookLib: filename=EbookLib-0.17.1-cp37-none-any.whl size=38187 sha256=700e3a7143f97062476b256a1090da62f2fffd99c904aec2af6c955ce3f74ef7\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/11/01/951369cbbf8f96878786a1f4da68bd7ac19a5d945b38e03d54\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.18-cp37-none-any.whl size=275504 sha256=33e77c19e42da9057fb4ca7c3f5f9ecff0b6cc40a993ac2396b7d1b3391e0728\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/1f/2c/29acca422b420a0b5210bd2cd7e9669804520d602d2462f20b\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-cp37-none-any.whl size=3981 sha256=c1d94a7bbc58eff4d96f279f95bfbe255cc6ec74cd1b8109bbc5cbd3d0d29d73\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35431 sha256=0e7ae3440ed79c6561098bda154c662e7538a69892e0c07eeae7103d25cc460d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built EbookLib python-pptx docx2txt olefile\n",
            "\u001b[31mERROR: tensorflow 1.15.2 has requirement gast==0.2.2, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: argcomplete, SpeechRecognition, six, EbookLib, XlsxWriter, python-pptx, imapclient, olefile, extract-msg, pycryptodome, pdfminer.six, soupsieve, beautifulsoup4, docx2txt, xlrd, textract\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed EbookLib-0.17.1 SpeechRecognition-3.8.1 XlsxWriter-1.4.3 argcomplete-1.10.0 beautifulsoup4-4.8.0 docx2txt-0.8 extract-msg-0.23.1 imapclient-2.1.0 olefile-0.46 pdfminer.six-20181108 pycryptodome-3.10.1 python-pptx-0.6.18 six-1.12.0 soupsieve-2.2.1 textract-1.6.3 xlrd-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-cp37-none-any.whl size=61102 sha256=208ba12ba260be233e56adb56ad4d8feed239984a8e25a21fecf0894a490ad5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDibLWj7ygay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad45eaea-caba-4cc3-e7d9-2a35f9e61955"
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxVGOo-UzOwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f81b58b8-4b00-4a80-9bb0-4cca5931ff88"
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nURZ2uaBv9yU"
      },
      "source": [
        "# A method to get predictions\n",
        "def getPrediction(in_sentences,predict_fn, type_output = \"features\"):\n",
        "  #A list to map the actual labels to the predictions\n",
        "  labels = lab\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] \n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  #Predicting the classes \n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in input_features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "  pred_dict = {\n",
        "    'input_ids': all_input_ids,\n",
        "    'input_mask': all_input_mask,\n",
        "    'segment_ids': all_segment_ids,\n",
        "    'label_ids': all_label_ids\n",
        "    }\n",
        "  #predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  #print(type(predict_input_fn))\n",
        "  predictions = predict_fn(pred_dict)\n",
        "  print([predictions['pooled_output']])\n",
        "  #print([enumerate(predictions)])\n",
        "  #print([pooled_output for prediction in enumerate(predictions['pooled_output'])])\n",
        " # print([pooled_output for prediction in enumerate(predictions['pooled_output'])].shape)\n",
        "  if type_output == \"features\":\n",
        "    return predictions['pooled_output']\n",
        "  else:\n",
        "    return [\n",
        "        (sentence, prediction, label)\n",
        "        for sentence, prediction, label in zip(pred_sentences, predictions['probabilities'], predictions['labels'])\n",
        "    ]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kDV3O9kZU4n"
      },
      "source": [
        "def val_generator(df):\n",
        "    x_list= df['emb'].to_list()\n",
        "    y_list =  df.label.to_list()\n",
        "    # Generate batches\n",
        "    while True:\n",
        "        for b in range(batches_per_epoch_val):\n",
        "            longest_index = (b + 1) * batch_size_val - 1\n",
        "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_val][-31:], key=len))\n",
        "            #print(len(df_train['emb'].to_list()[:b+batch_size][-7:]))\n",
        "            x_train = np.full((batch_size_val, timesteps, num_features), -99.)\n",
        "            y_train = np.zeros((batch_size_val,  1))\n",
        "            for i in range(batch_size_val):\n",
        "                li = b * batch_size_val + i\n",
        "                #print(\"li\", li)\n",
        "                #print(x_train[i, 0:len(x_list[li]), :].shape, len(x_list[li]))\n",
        "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
        "                y_train[i] = y_list[li]\n",
        "            yield x_train, y_train"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxqx5matfoXb"
      },
      "source": [
        "Sample Documents\n",
        "(Place all the documents to be classified under /content)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMq9OBKXfJxi",
        "outputId": "5adc4a20-94a2-4ea9-906b-47f3837571ac"
      },
      "source": [
        "!gdown --id 10KS_d5Yqaw2VbRQQ8SpZuGMwPZpvBWXw"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10KS_d5Yqaw2VbRQQ8SpZuGMwPZpvBWXw\n",
            "To: /content/Letter-Abdul Mannan Zafar.pdf\n",
            "\r  0% 0.00/103k [00:00<?, ?B/s]\r100% 103k/103k [00:00<00:00, 29.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTvpvJS6f_ol",
        "outputId": "5c9d5a11-e7d7-4403-baff-a5aae99a7d21"
      },
      "source": [
        "!gdown --id 1tgQxek831nw_U4GTSiYBSF8VPaoBmDgQ"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tgQxek831nw_U4GTSiYBSF8VPaoBmDgQ\n",
            "To: /content/Technical Seminar Synopsis - Abdul Mannan Zafar.pdf\n",
            "\r  0% 0.00/578k [00:00<?, ?B/s]\r100% 578k/578k [00:00<00:00, 38.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ddh0lwLenZv"
      },
      "source": [
        "### Document Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkH4bewV4-rF"
      },
      "source": [
        "import os\n",
        "import textract\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "\n",
        "# file to store the data\n",
        "file=open(\"projeg.txt\",'w')\n",
        "\n",
        "#basepath is the present directory scanned\n",
        "basepath = './'\n",
        "\n",
        "doclst=[[\"name\",\"filedata\"]]\n",
        "\n",
        "doc=[\"doc\",\"docx\",\"html\",\"htm\",\"txt\"]\n",
        "\n",
        "#printing the files scanned and the data is stored in the \"file\"\n",
        "with os.scandir(basepath) as entries:\n",
        "\tfor entry in entries:\n",
        "\t\tif entry.is_file():\n",
        "\t\t\tname=(entry.name).split(\".\")\n",
        "\t\t\tif name[1] in doc:\n",
        "\t\t\t\tdoclst.append([(basepath + entry.name),((textract.process(basepath+entry.name)).decode())])\n",
        "\t\t\tif name[1] == \"pdf\":\n",
        "\t\t\t\tpagedata=\"\"\n",
        "\t\t\t\twith open((basepath + entry.name),'rb') as pdf_file:\n",
        "\t\t\t\t\tread_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
        "\t\t\t\t\tfor page_number in range(read_pdf.getNumPages()):\n",
        "\t\t\t\t\t\tpage = read_pdf.getPage(page_number)\n",
        "\t\t\t\t\t\tpagedata += page.extractText()\n",
        "\t\t\t\tdoclst.append([(basepath + entry.name),(pagedata)])\n",
        "for i in doclst:\n",
        "\tfile.write(\"\\n\" + i[0] + \"\\n\" + i[1])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoWexaG9QSG8"
      },
      "source": [
        "fname=[]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNrd9Wke1Je"
      },
      "source": [
        "### Confidential Document Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wc1LFfm5bi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981edac7-e9b1-49fd-8404-92612e4623fb"
      },
      "source": [
        "pred_sentences= []\n",
        "i=0\n",
        "for x in doclst:\n",
        "  pred_sentences=x\n",
        "  pred_sentences[1]=pred_sentences[1].replace(\"\\n\",\" \")\n",
        "  print(pred_sentences[1])\n",
        "  #pred_sentences.append(doclst[i])\n",
        "  #pred_sentences.append(doclst[i+1])\n",
        "  #i=i+2\n",
        "  tst_label= [0]*len(pred_sentences)\n",
        "  test = pd.DataFrame({'text':pred_sentences, 'label':tst_label, })\n",
        "  test['text_split']=test[\"text\"].apply(get_split)\n",
        "\n",
        "  test_l = []\n",
        "  test_label_l = []\n",
        "  test_index_l = []\n",
        "  for idx,row in test.iterrows():\n",
        "    for l in row['text_split']:\n",
        "      test_l.append(l)\n",
        "      test_label_l.append(row['label'])\n",
        "      test_index_l.append(idx)\n",
        "  len(test_l), len(test_label_l), len(test_index_l)\n",
        "  test_df = pd.DataFrame({DATA_COLUMN:test_l, LABEL_COLUMN:test_label_l})\n",
        "  print(test_df.head())\n",
        "  flag=0\n",
        "  for x in imp:\n",
        "    if(x in pred_sentences[1].lower()):\n",
        "      flag=1\n",
        "  #model = tf.compat.v2.saved_model.load(str('/content/bert_news_category/1596907431'), None)\n",
        "  #  model = model.signatures[\"predict\"]\n",
        "  test_emb = np.apply_along_axis(getPrediction,0,np.array(test_df[DATA_COLUMN]),predict_fn)\n",
        "  print(test_emb.shape)\n",
        "  aux = -1\n",
        "  len_l = 0\n",
        "  test_x = {}\n",
        "\n",
        "  for l, emb in zip(test_index_l, test_emb):\n",
        "    if l in test_x.keys():\n",
        "      test_x[l]  =np.vstack([test_x[l], emb])\n",
        "    else:\n",
        "      test_x[l] = [emb]\n",
        "\n",
        "\n",
        "  test_l_final = []\n",
        "  tlabel_l_final = []\n",
        "  for k in test_x.keys():\n",
        "    test_l_final.append(test_x[k])\n",
        "    tlabel_l_final.append(test.loc[k]['label'])\n",
        "\n",
        "  df_test = pd.DataFrame({'emb': test_l_final, 'label': tlabel_l_final})\n",
        "  print(df_test.head())\n",
        "  print(df_test.shape)\n",
        "  num_sequences_val = len(df_test['emb'].to_list())\n",
        "  batch_size_val = 1\n",
        "  batches_per_epoch_val = 2\n",
        "  assert batch_size_val * batches_per_epoch_val == num_sequences_val\n",
        "  num_features= 768\n",
        "  res1=model.predict_generator(val_generator(df_test), steps=batches_per_epoch_val)\n",
        "  print(res1)\n",
        "  lst=[]\n",
        "  for x in res1:\n",
        "    lst.append(x.tolist().index(max(x)))\n",
        "  df_test['label']=lst\n",
        "  conf=[0]*len(lst)\n",
        "\n",
        "  for i in range(0,len(lst)):\n",
        "    if(lst[i]==0 or lst[i]==2 or lst[i]==3 or lst[i]==5 or lst[i]==7 or flag==1):\n",
        "      conf[i]=1\n",
        "  if(conf[1]):\n",
        "    fname.append(pred_sentences[0])\n",
        "  df_test['confidentiality']=conf\n",
        "  print(df_test)\n",
        "  df = pd.DataFrame(None)\n",
        "  lst.clear()\n",
        "  conf.clear()\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filedata\n",
            "       text  label\n",
            "0      name      0\n",
            "1  filedata      0\n",
            "INFO:tensorflow:Writing example 0 of 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] name [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] name [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2171 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2171 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] filed ##ata [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] filed ##ata [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6406 6790 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6406 6790 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[array([[-0.72714096, -0.3105129 ,  0.89079547, ...,  0.49353907,\n",
            "        -0.4286952 ,  0.8529303 ],\n",
            "       [-0.5979898 , -0.5477721 ,  0.9726995 , ...,  0.51200575,\n",
            "        -0.5045193 ,  0.8182703 ]], dtype=float32)]\n",
            "(2, 768)\n",
            "                                                 emb  label\n",
            "0  [[-0.72714096, -0.3105129, 0.89079547, 0.59150...      0\n",
            "1  [[-0.5979898, -0.5477721, 0.9726995, 0.5043299...      0\n",
            "(2, 2)\n",
            "[[4.2325687e-03 6.8485700e-02 7.1619847e-03 3.5541955e-02 3.0077016e-02\n",
            "  1.7117921e-04 8.4780926e-01 4.8044696e-03 4.0454892e-04 1.3112887e-03]\n",
            " [2.1242977e-03 1.2192447e-02 7.3101890e-04 6.3142449e-02 4.0135514e-02\n",
            "  2.8058744e-04 8.7686765e-01 3.6272109e-03 1.2405160e-04 7.7482208e-04]]\n",
            "                                                 emb  label  confidentiality\n",
            "0  [[-0.72714096, -0.3105129, 0.89079547, 0.59150...      6                0\n",
            "1  [[-0.5979898, -0.5477721, 0.9726995, 0.5043299...      6                0\n",
            " name filedata ./projeg.txt  ./CoverLetter_AbdulMannanZafar.pdf  ./Technical Seminar Synopsis - Abdul Mannan Zafar.pdf \n",
            "                                                text  label\n",
            "0                                       ./projeg.txt      0\n",
            "1  name filedata ./projeg.txt ./CoverLetter_Abdul...      0\n",
            "INFO:tensorflow:Writing example 0 of 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . / pro ##je ##g . tx ##t [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . / pro ##je ##g . tx ##t [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1013 4013 6460 2290 1012 19067 2102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1013 4013 6460 2290 1012 19067 2102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] name filed ##ata . / pro ##je ##g . tx ##t . / cover ##lette ##r _ abdul ##mann ##anza ##far . pdf . / technical seminar syn ##opsis - abdul mann ##an za ##far . pdf [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] name filed ##ata . / pro ##je ##g . tx ##t . / cover ##lette ##r _ abdul ##mann ##anza ##far . pdf . / technical seminar syn ##opsis - abdul mann ##an za ##far . pdf [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2171 6406 6790 1012 1013 4013 6460 2290 1012 19067 2102 1012 1013 3104 27901 2099 1035 10298 5804 16076 14971 1012 11135 1012 1013 4087 18014 19962 22599 1011 10298 10856 2319 23564 14971 1012 11135 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2171 6406 6790 1012 1013 4013 6460 2290 1012 19067 2102 1012 1013 3104 27901 2099 1035 10298 5804 16076 14971 1012 11135 1012 1013 4087 18014 19962 22599 1011 10298 10856 2319 23564 14971 1012 11135 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[array([[-0.8441546 , -0.47155306,  0.35794917, ..., -0.3707298 ,\n",
            "        -0.4354202 ,  0.8990495 ],\n",
            "       [-0.45559108, -0.7931717 ,  0.3930699 , ..., -0.6486491 ,\n",
            "         0.00221256,  0.8881761 ]], dtype=float32)]\n",
            "(2, 768)\n",
            "                                                 emb  label\n",
            "0  [[-0.8441546, -0.47155306, 0.35794917, 0.76906...      0\n",
            "1  [[-0.45559108, -0.7931717, 0.3930699, 0.584634...      0\n",
            "(2, 2)\n",
            "[[8.1458325e-03 4.6188004e-02 1.6691774e-02 1.7536502e-01 5.2203983e-02\n",
            "  1.1582155e-03 6.9416499e-01 5.2960436e-03 4.7527152e-05 7.3858636e-04]\n",
            " [1.6863848e-03 3.9660946e-02 3.4927928e-03 1.7453180e-01 3.5976520e-01\n",
            "  3.4240179e-04 4.1129342e-01 6.3343071e-03 3.0756925e-04 2.5851834e-03]]\n",
            "                                                 emb  label  confidentiality\n",
            "0  [[-0.8441546, -0.47155306, 0.35794917, 0.76906...      6                0\n",
            "1  [[-0.45559108, -0.7931717, 0.3930699, 0.584634...      6                0\n",
            "Dear Evlin Ma'am,     Hope you're doing well and safe. I am writing this letter in regards with a referral request. I  recently applied for the role of Student Intern at Microsoft. Keeping in mind all the great efforts  you have put in as our teacher and a  mentor, you have my utmost appreciation and gratitude. As  your teaching has inspired me immensely, I was hoping that you might know me well enough  and have a high enough regard for my abilities to provide a recommendation.   I am listing all the skills, proj ects and the courses I have completed, along with my resume.     Skills :   Languages:    Python, C, C++, Java, HTML, SQL, CSS   Technical skills:    Machine Learning, Natural Language Processing, Deep Learning, Computer Networks,  Application/Web Development, Database   Management, Project Management.   Soft Skills:    Competitive| Leadership| Persuasive| Verbal and Written Communication| Multilingual |  Documentation | A/V Editing.     Projects :     SECURE NUMBER.   A software prototype which detects an ongoing fraudulent phone call . This project was  developed using NLP and a speech to text API. User is alerted if the python script predicts the  call as fraudulent.     SENTIMENT   ANALYSIS .   Simple Sentiment classification of IMDB movie reviews using the NLP model BERT.   Classifies movie rev iews into distinct classes based on the sentiments expressed by the  reviewer.   Accuracy: 87%       LIBRARY MANAGEMENT SYSTEM.   Database Management for a College Library with front end made using JAVA and Backend  using My - SQL connected by a JDBC interface.     VEHIC LE ACCIDENT LOCATION TRACKER   Arduino based project that sends the GPS coordinates of the vehicle's location to emergency  services via GSM SMS, when it detects an accident.     PRICE PREDICTION OF USED CARS.   Submitted as a Solution to a Google DSC  Challenge. Used Random Forest Regression with cross   validation.   Accuracy: Train - 98% Test - 91% obtained by necessary feature selection and feature  importance.     FIREWORKS 3D ANIMATION.   Submitted as a project for Computer Graphics Laboratory.   Simulation of 3D  fireworks using particle effect concept in OpenGl. Animated fireworks  developed as a result of multiple particle explosions, along with custom camera perspective  and user interactions to control the movement of the fireworks 3 Dimensionally.     Courses:       - outs Campus Edition  -   Completed and earned 3 badges.    -   Coursera.      Theory and Practice, organised by The Pes  Centre for pattern recognition and Department of CSE.     I conclude my humble request here. Hope to receive a positive feedback.     Thanking you,   Yours sincerely,   Abdul Mannan Zafar.   \n",
            "                                                text  label\n",
            "0                    ./Letter-Abdul Mannan Zafar.pdf      0\n",
            "1  Dear Evlin Ma'am, Hope you're doing well and s...      0\n",
            "2  software prototype which detects an ongoing fr...      0\n",
            "INFO:tensorflow:Writing example 0 of 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . / letter - abdul mann ##an za ##far . pdf [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . / letter - abdul mann ##an za ##far . pdf [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1013 3661 1011 10298 10856 2319 23564 14971 1012 11135 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1013 3661 1011 10298 10856 2319 23564 14971 1012 11135 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] dear ev ##lin ma ' am , hope you ' re doing well and safe . i am writing this letter in regards with a refer ##ral request . i recently applied for the role of student intern at microsoft . keeping in mind all the great efforts you have put in as our teacher and a mentor , you have my utmost appreciation and gratitude . as your teaching has inspired me immensely , i was hoping that you might know me well enough and have a high enough regard for my abilities to provide a recommendation . i am listing all the skills , pro ##j ec ##ts and the courses i have completed , along with my resume . skills : languages : python , c , c + + , java , html , sql , cs ##s technical skills : machine learning , natural language processing , deep learning , computer networks , application / web development , database management , project management . soft skills : competitive | leadership | per ##su ##asi ##ve | verbal and written communication | multi ##ling ##ual | documentation | a / v editing . projects [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] dear ev ##lin ma ' am , hope you ' re doing well and safe . i am writing this letter in regards with a refer ##ral request . i recently applied for the role of student intern at microsoft . keeping in mind all the great efforts you have put in as our teacher and a mentor , you have my utmost appreciation and gratitude . as your teaching has inspired me immensely , i was hoping that you might know me well enough and have a high enough regard for my abilities to provide a recommendation . i am listing all the skills , pro ##j ec ##ts and the courses i have completed , along with my resume . skills : languages : python , c , c + + , java , html , sql , cs ##s technical skills : machine learning , natural language processing , deep learning , computer networks , application / web development , database management , project management . soft skills : competitive | leadership | per ##su ##asi ##ve | verbal and written communication | multi ##ling ##ual | documentation | a / v editing . projects [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6203 23408 4115 5003 1005 2572 1010 3246 2017 1005 2128 2725 2092 1998 3647 1012 1045 2572 3015 2023 3661 1999 12362 2007 1037 6523 7941 5227 1012 1045 3728 4162 2005 1996 2535 1997 3076 25204 2012 7513 1012 4363 1999 2568 2035 1996 2307 4073 2017 2031 2404 1999 2004 2256 3836 1998 1037 10779 1010 2017 2031 2026 27917 12284 1998 15531 1012 2004 2115 4252 2038 4427 2033 24256 1010 1045 2001 5327 2008 2017 2453 2113 2033 2092 2438 1998 2031 1037 2152 2438 7634 2005 2026 7590 2000 3073 1037 12832 1012 1045 2572 10328 2035 1996 4813 1010 4013 3501 14925 3215 1998 1996 5352 1045 2031 2949 1010 2247 2007 2026 13746 1012 4813 1024 4155 1024 18750 1010 1039 1010 1039 1009 1009 1010 9262 1010 16129 1010 29296 1010 20116 2015 4087 4813 1024 3698 4083 1010 3019 2653 6364 1010 2784 4083 1010 3274 6125 1010 4646 1013 4773 2458 1010 7809 2968 1010 2622 2968 1012 3730 4813 1024 6975 1064 4105 1064 2566 6342 21369 3726 1064 12064 1998 2517 4807 1064 4800 2989 8787 1064 12653 1064 1037 1013 1058 9260 1012 3934 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6203 23408 4115 5003 1005 2572 1010 3246 2017 1005 2128 2725 2092 1998 3647 1012 1045 2572 3015 2023 3661 1999 12362 2007 1037 6523 7941 5227 1012 1045 3728 4162 2005 1996 2535 1997 3076 25204 2012 7513 1012 4363 1999 2568 2035 1996 2307 4073 2017 2031 2404 1999 2004 2256 3836 1998 1037 10779 1010 2017 2031 2026 27917 12284 1998 15531 1012 2004 2115 4252 2038 4427 2033 24256 1010 1045 2001 5327 2008 2017 2453 2113 2033 2092 2438 1998 2031 1037 2152 2438 7634 2005 2026 7590 2000 3073 1037 12832 1012 1045 2572 10328 2035 1996 4813 1010 4013 3501 14925 3215 1998 1996 5352 1045 2031 2949 1010 2247 2007 2026 13746 1012 4813 1024 4155 1024 18750 1010 1039 1010 1039 1009 1009 1010 9262 1010 16129 1010 29296 1010 20116 2015 4087 4813 1024 3698 4083 1010 3019 2653 6364 1010 2784 4083 1010 3274 6125 1010 4646 1013 4773 2458 1010 7809 2968 1010 2622 2968 1012 3730 4813 1024 6975 1064 4105 1064 2566 6342 21369 3726 1064 12064 1998 2517 4807 1064 4800 2989 8787 1064 12653 1064 1037 1013 1058 9260 1012 3934 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] software prototype which detect ##s an ongoing fraudulent phone call . this project was developed using nl ##p and a speech to text api . user is alerted if the python script predict ##s the call as fraudulent . sentiment analysis . simple sentiment classification of im ##db movie reviews using the nl ##p model bert . class ##ifies movie rev ie ##ws into distinct classes based on the sentiments expressed by the reviewer . accuracy : 87 % library management system . database management for a college library with front end made using java and back ##end using my - sql connected by a jd ##bc interface . ve ##hic le accident location tracker ar ##du ##ino based project that sends the gps coordinates of the vehicle ' s location to emergency services via gs ##m sms , when it detect ##s an accident . price prediction of used cars . submitted as a solution to a google ds ##c challenge . used random forest regression with cross validation . accuracy : train - 98 % test - 91 % obtained by necessary feature selection and feature importance . fireworks 3d animation . submitted as a project [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] software prototype which detect ##s an ongoing fraudulent phone call . this project was developed using nl ##p and a speech to text api . user is alerted if the python script predict ##s the call as fraudulent . sentiment analysis . simple sentiment classification of im ##db movie reviews using the nl ##p model bert . class ##ifies movie rev ie ##ws into distinct classes based on the sentiments expressed by the reviewer . accuracy : 87 % library management system . database management for a college library with front end made using java and back ##end using my - sql connected by a jd ##bc interface . ve ##hic le accident location tracker ar ##du ##ino based project that sends the gps coordinates of the vehicle ' s location to emergency services via gs ##m sms , when it detect ##s an accident . price prediction of used cars . submitted as a solution to a google ds ##c challenge . used random forest regression with cross validation . accuracy : train - 98 % test - 91 % obtained by necessary feature selection and feature importance . fireworks 3d animation . submitted as a project [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4007 8773 2029 11487 2015 2019 7552 27105 3042 2655 1012 2023 2622 2001 2764 2478 17953 2361 1998 1037 4613 2000 3793 17928 1012 5310 2003 22333 2065 1996 18750 5896 16014 2015 1996 2655 2004 27105 1012 15792 4106 1012 3722 15792 5579 1997 10047 18939 3185 4391 2478 1996 17953 2361 2944 14324 1012 2465 14144 3185 7065 29464 9333 2046 5664 4280 2241 2006 1996 23541 5228 2011 1996 12027 1012 10640 1024 6584 1003 3075 2968 2291 1012 7809 2968 2005 1037 2267 3075 2007 2392 2203 2081 2478 9262 1998 2067 10497 2478 2026 1011 29296 4198 2011 1037 26219 9818 8278 1012 2310 16066 3393 4926 3295 27080 12098 8566 5740 2241 2622 2008 10255 1996 14658 12093 1997 1996 4316 1005 1055 3295 2000 5057 2578 3081 28177 2213 22434 1010 2043 2009 11487 2015 2019 4926 1012 3976 17547 1997 2109 3765 1012 7864 2004 1037 5576 2000 1037 8224 16233 2278 4119 1012 2109 6721 3224 26237 2007 2892 27354 1012 10640 1024 3345 1011 5818 1003 3231 1011 6205 1003 4663 2011 4072 3444 4989 1998 3444 5197 1012 16080 7605 7284 1012 7864 2004 1037 2622 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4007 8773 2029 11487 2015 2019 7552 27105 3042 2655 1012 2023 2622 2001 2764 2478 17953 2361 1998 1037 4613 2000 3793 17928 1012 5310 2003 22333 2065 1996 18750 5896 16014 2015 1996 2655 2004 27105 1012 15792 4106 1012 3722 15792 5579 1997 10047 18939 3185 4391 2478 1996 17953 2361 2944 14324 1012 2465 14144 3185 7065 29464 9333 2046 5664 4280 2241 2006 1996 23541 5228 2011 1996 12027 1012 10640 1024 6584 1003 3075 2968 2291 1012 7809 2968 2005 1037 2267 3075 2007 2392 2203 2081 2478 9262 1998 2067 10497 2478 2026 1011 29296 4198 2011 1037 26219 9818 8278 1012 2310 16066 3393 4926 3295 27080 12098 8566 5740 2241 2622 2008 10255 1996 14658 12093 1997 1996 4316 1005 1055 3295 2000 5057 2578 3081 28177 2213 22434 1010 2043 2009 11487 2015 2019 4926 1012 3976 17547 1997 2109 3765 1012 7864 2004 1037 5576 2000 1037 8224 16233 2278 4119 1012 2109 6721 3224 26237 2007 2892 27354 1012 10640 1024 3345 1011 5818 1003 3231 1011 6205 1003 4663 2011 4072 3444 4989 1998 3444 5197 1012 16080 7605 7284 1012 7864 2004 1037 2622 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[array([[-0.8596721 , -0.77015543, -0.7877585 , ..., -0.88319576,\n",
            "        -0.57534534,  0.910743  ],\n",
            "       [ 0.21486844, -0.50395983,  0.1389877 , ..., -0.6968766 ,\n",
            "        -0.7863148 ,  0.26663515],\n",
            "       [ 0.27285302, -0.16680238,  0.70615065, ..., -0.9280138 ,\n",
            "         0.04256385,  0.37611437]], dtype=float32)]\n",
            "(3, 768)\n",
            "                                                 emb  label\n",
            "0  [[-0.8596721, -0.77015543, -0.7877585, 0.90956...      0\n",
            "1  [[0.21486844, -0.50395983, 0.1389877, 0.161801...      0\n",
            "(2, 2)\n",
            "[[1.3129972e-03 5.4275341e-02 1.2984282e-03 3.7654504e-02 2.0019227e-01\n",
            "  3.9907075e-02 1.4628252e-01 2.2880811e-02 5.2890956e-04 4.9566719e-01]\n",
            " [6.8817637e-04 3.2238420e-02 2.6571800e-04 6.4998621e-01 1.8992357e-01\n",
            "  3.2461795e-04 8.7424345e-02 6.1329589e-03 2.0493555e-06 3.3013947e-02]]\n",
            "                                                 emb  label  confidentiality\n",
            "0  [[-0.8596721, -0.77015543, -0.7877585, 0.90956...      9                1\n",
            "1  [[0.21486844, -0.50395983, 0.1389877, 0.161801...      3                1\n",
            "\n",
            "                                                text  label\n",
            "0  ./Technical Seminar Synopsis - Abdul Mannan Za...      0\n",
            "1                                                         0\n",
            "INFO:tensorflow:Writing example 0 of 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . / technical seminar syn ##opsis - abdul mann ##an za ##far . pdf [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . / technical seminar syn ##opsis - abdul mann ##an za ##far . pdf [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1013 4087 18014 19962 22599 1011 10298 10856 2319 23564 14971 1012 11135 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1013 4087 18014 19962 22599 1011 10298 10856 2319 23564 14971 1012 11135 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[array([[-0.54559904, -0.6514454 , -0.2312796 , ..., -0.87588143,\n",
            "        -0.44856656,  0.7901454 ],\n",
            "       [-0.8345141 , -0.35685617,  0.8250994 , ...,  0.43714058,\n",
            "         0.1113009 ,  0.90122503]], dtype=float32)]\n",
            "(2, 768)\n",
            "                                                 emb  label\n",
            "0  [[-0.54559904, -0.6514454, -0.2312796, 0.66368...      0\n",
            "1  [[-0.8345141, -0.35685617, 0.8250994, 0.838091...      0\n",
            "(2, 2)\n",
            "[[6.7042804e-04 1.0497753e-01 1.8855763e-03 2.3991134e-02 1.1905316e-01\n",
            "  5.1501542e-03 8.5063078e-02 7.2259055e-03 4.6771517e-04 6.5151536e-01]\n",
            " [2.6370291e-02 1.8897098e-01 4.7629863e-02 7.8878626e-02 9.9185646e-02\n",
            "  9.3427266e-04 5.3649002e-01 1.4814585e-02 1.7819608e-03 4.9438435e-03]]\n",
            "                                                 emb  label  confidentiality\n",
            "0  [[-0.54559904, -0.6514454, -0.2312796, 0.66368...      9                0\n",
            "1  [[-0.8345141, -0.35685617, 0.8250994, 0.838091...      6                0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL9s64WtJIep"
      },
      "source": [
        "df_output=pd.DataFrame(columns=['conf_file'])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ewItj6Fzmu"
      },
      "source": [
        "### List of Confidential Documents extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMygt8AmQzcM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "181a27e6-50c2-418e-81ea-29caf30e3dbe"
      },
      "source": [
        "df_output['conf_file']=fname\n",
        "df_output"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conf_file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./Letter-Abdul Mannan Zafar.pdf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         conf_file\n",
              "0  ./Letter-Abdul Mannan Zafar.pdf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}